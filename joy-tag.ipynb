{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10157801,"sourceType":"datasetVersion","datasetId":6271868}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install accelerate","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport tarfile\nimport torch\nfrom PIL import Image\nfrom transformers import AutoProcessor, LlavaForConditionalGeneration\nimport piexif\nfrom accelerate import Accelerator\nimport torchvision.transforms.functional as TVF\n\n# 绝对路径\nMODEL_NAME = \"fancyfeast/llama-joycaption-alpha-two-hf-llava\"\nTAR_INPUT_PATH = \"\"  # 输入tar文件的绝对路径\nTAR_OUTPUT_PATH = \"\"    # 输出tar文件的绝对路径\nTEMP_DIR = \"\"       # 临时解压目录的绝对路径\n\nprompt = f'''Write a stable diffusion prompt for this image within 150 words. Focus on the subjects and their physical traits and physical interactions. \nDo NOT name series or character names. \nDo NOT use any ambiguous language. \nDo NOT use tags or lists. \nDo NOT describe the mood or atmosphere. \nDo NOT describe emotions. \nIf applicable, describe sexual scenes or nudity with informal, sexual slang. \nIf a vagina (pussy), penis, butt (ass), nipples or sexual fluid (cum) is present, describe them with informal, sexual slang. \nIf characters are engaging in sex (fucking), describe the position and type of sex with informal, sexual slang. \nIf there are multiple named characters in the image, differentiate each character based on its features, and use character names instead of personal pronouns.\n'''\nif not os.path.exists(TEMP_DIR): os.makedirs(TEMP_DIR)\naccelerator = Accelerator()\nprocessor = AutoProcessor.from_pretrained(MODEL_NAME)\ndevice_map = \"auto\" if torch.cuda.is_available() else None\nllava_model = LlavaForConditionalGeneration.from_pretrained(MODEL_NAME, torch_dtype=torch.bfloat16, device_map=device_map)\nllava_model.eval()\nllava_model, processor = accelerator.prepare(llava_model, processor)\nwith tarfile.open(TAR_INPUT_PATH, 'r') as tar: tar.extractall(path=TEMP_DIR)\nimage_files = [os.path.join(TEMP_DIR, f) for f in os.listdir(TEMP_DIR) if f.lower().endswith(('png', 'jpg', 'jpeg'))]\nwith tarfile.open(TAR_OUTPUT_PATH, 'w') as tar_out:\n    with torch.no_grad():\n        for image_path in image_files:\n            try:\n                print(f\"Processing {image_path}\")\n                image = Image.open(image_path).convert(\"RGB\")\n                if image.size != (384, 384):\n                    image = image.resize((384, 384), Image.LANCZOS)\n                pixel_values = TVF.pil_to_tensor(image)\n                pixel_values = pixel_values / 255.0\n                pixel_values = TVF.normalize(pixel_values, [0.5], [0.5])\n                pixel_values = pixel_values.to(torch.bfloat16).unsqueeze(0).to(accelerator.device)\n                convo = [\n                    {\"role\": \"system\", \"content\": \"You are a helpful image captioner.\"},\n                    {\"role\": \"user\", \"content\": prompt}\n                ]\n                convo_string = processor.apply_chat_template(convo, tokenize=False, add_generation_prompt=True)\n                assert isinstance(convo_string, str)\n                inputs = processor(text=[convo_string], images=[image], return_tensors=\"pt\")\n                inputs = {k: v.to(accelerator.device) for k, v in inputs.items()}\n                inputs['pixel_values'] = inputs['pixel_values'].to(torch.bfloat16)\n                generate_ids = llava_model.generate(\n                    **inputs,\n                    max_new_tokens=300,\n                    do_sample=True,\n                    suppress_tokens=None,\n                    use_cache=True,\n                    temperature=0.6,\n                    top_k=None,\n                    top_p=0.9,\n                )[0]\n                generate_ids = generate_ids[inputs['input_ids'].shape[1]:]\n                caption = processor.tokenizer.decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False).strip()\n                print(f\"Generated Caption for {image_path}: {caption}\")\n                exif_dict = piexif.load(image.info.get(\"exif\", b\"\"))\n                user_comment = caption.encode(\"utf-8\")\n                exif_dict[\"Exif\"][piexif.ExifIFD.UserComment] = piexif.helper.UserComment.dump(user_comment, encoding=\"unicode\")\n                exif_bytes = piexif.dump(exif_dict)\n                output_image_path = os.path.join(TEMP_DIR, os.path.basename(image_path))\n                image.save(output_image_path, \"jpeg\", exif=exif_bytes)\n                tar_out.add(output_image_path, arcname=os.path.basename(image_path))\n            except Exception as e:\n                print(f\"Failed to process {image_path}: {e}\")\nprint(f\"Processed images saved to {TAR_OUTPUT_PATH}\")\nfor file in os.listdir(TEMP_DIR):\n    file_path = os.path.join(TEMP_DIR, file)\n    if os.path.isfile(file_path): os.remove(file_path)\nos.rmdir(TEMP_DIR)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}